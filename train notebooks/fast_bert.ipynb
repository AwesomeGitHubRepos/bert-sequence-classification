{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"./data\")\n",
    "LABEL_PATH = Path(\"./labels\")\n",
    "OUTPUT_DIR = Path(\"./output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.data_cls import BertDataBunch\n",
    "\n",
    "labels_list = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH,\n",
    "                          tokenizer='distilbert-base-cased',\n",
    "                          train_file='train_smaller.csv',\n",
    "                          val_file='valid.csv',\n",
    "                          label_file='labels.csv',\n",
    "                          text_col='comment_text',\n",
    "                          label_col=labels_list,\n",
    "                          batch_size_per_gpu=16,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=True,\n",
    "                          model_type='distilbert')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy, roc_auc, fbeta\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "metrics = [{'name': 'accuracy', 'function': accuracy}]\n",
    "metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "        databunch,\n",
    "        pretrained_path='distilbert-base-cased',\n",
    "        metrics=metrics,\n",
    "        device=device,\n",
    "        logger=logger,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        finetuned_wgts_path=None,\n",
    "        warmup_steps=500,\n",
    "        multi_gpu=False,\n",
    "        is_fp16=True,\n",
    "        multi_label=True,\n",
    "        logging_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs=5,\n",
    "        lr=6e-5,\n",
    "        validate=True, # Evaluate the model after each epoch\n",
    "        schedule_type=\"warmup_cosine\",\n",
    "        optimizer_type=\"lamb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['you motherfucker, i am going to kill you',\n",
    "         'this is a nice comment, i love you so much']\n",
    "predictions = learner.predict_batch(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for text in predictions:\n",
    "    print(f\"Prediction for sentence: {texts[i]}:\")\n",
    "    i+=1\n",
    "    for pred in text:\n",
    "        print(pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction object for when a fine-tuned model isn't in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./output/model_out\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "MODEL_PATH = \"./output/model_out\"\n",
    "\n",
    "predictor = BertClassificationPredictor(\n",
    "            model_path=MODEL_PATH,\n",
    "            label_path=LABEL_PATH, # location for labels.csv file\n",
    "            multi_label=True,\n",
    "            model_type='distilbert',\n",
    "            do_lower_case=False)\n",
    "\n",
    "# Single prediction\n",
    "single_text = \"i hate you\"\n",
    "single_prediction = predictor.predict(single_text)\n",
    "\n",
    "# Batch predictions\n",
    "multiple_texts = ['i am going to kill you',\n",
    "         'this is a nice comment, i love you so much']\n",
    "\n",
    "multiple_predictions = predictor.predict_batch(multiple_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for sentence: i hate you:\n",
      "('toxic', 0.7379467487335205)\n",
      "('insult', 0.29986605048179626)\n",
      "('obscene', 0.05977706238627434)\n",
      "('identity_hate', 0.03005264513194561)\n",
      "('threat', 0.021713832393288612)\n",
      "('severe_toxic', 0.0054395017214119434)\n"
     ]
    }
   ],
   "source": [
    "print(f\"prediction for sentence: {single_text}:\")\n",
    "for pred in single_prediction:\n",
    "    print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for sentence: i am going to kill you:\n",
      "('toxic', 0.8307578563690186)\n",
      "('insult', 0.3980618715286255)\n",
      "('obscene', 0.10295048356056213)\n",
      "('identity_hate', 0.056071922183036804)\n",
      "('threat', 0.04543887451291084)\n",
      "('severe_toxic', 0.010167397558689117)\n",
      "\n",
      "Prediction for sentence: this is a nice comment, i love you so much:\n",
      "('toxic', 0.0031685384456068277)\n",
      "('obscene', 0.0009596769232302904)\n",
      "('insult', 0.000811850477475673)\n",
      "('identity_hate', 0.00038552848855033517)\n",
      "('threat', 0.00035436335019767284)\n",
      "('severe_toxic', 0.00014609545178245753)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for text in multiple_predictions:\n",
    "    print(f\"Prediction for sentence: {multiple_texts[i]}:\")\n",
    "    i+=1\n",
    "    for pred in text:\n",
    "        print(pred)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
